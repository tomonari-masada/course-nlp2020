{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_document_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXxfgG9mz7/0jCbiXbEQD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course-nlp2020/blob/master/07_document_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0ZZ8bo1mgH"
      },
      "source": [
        "# 07 単語埋め込みを使った文書分類\n",
        "* 今回も、IMDbデータセットの感情分析を文書分類問題として解く。\n",
        "* ただし今回は、fastTextのような学習済みの単語埋め込みは使わない。\n",
        "* 単語埋め込み自体の学習も、ネットワークの学習と同時におこなう。\n",
        "* IMDbデータの準備も、`torch.torchtext`を使っておこなう。\n",
        " * つまりすべてをPyTorchのなかでおこなう。\n",
        "* 参考資料\n",
        " * https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/1%20-%20Simple%20Sentiment%20Analysis.ipynb\n",
        " * https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyjU004LNbMt"
      },
      "source": [
        "## データをどう扱うか\n",
        "* ネットワークへの入力は、単語埋め込みを、単語の出現順どおりに並べた列にする。\n",
        " * ミニバッチは[ミニバッチのなかでの最大文書長, ミニバッチのサイズ, 単語埋め込み次元数]という形の3階のテンソルになる。\n",
        "* そして、前向き計算のなかではじめて、単語埋め込みの平均をとることにする。\n",
        " * `.mean(0)`と、軸0で平均をとることになる。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_puYg6Zi8x3"
      },
      "source": [
        "## 07-00 Google Colabのランタイムのタイプを変更する\n",
        "* Google ColabのランタイムのタイプをGPUに変更しておこう。\n",
        " * 上のメニューの「ランタイム」→「ランタイムのタイプを変更」→「ハードウェア　アクセラレータ」から「GPU」を選択"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLEeO0fw23Xp"
      },
      "source": [
        "## 07-01 torchtextを使ってIMDbデータを読み込む\n",
        "* ここでIMDbデータセットの読み込みにつかう`torchtext.datasets`については、下記を参照。\n",
        " * https://torchtext.readthedocs.io/en/latest/datasets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go7epLZe3JmF"
      },
      "source": [
        "### 実験の再現性確保のための設定など\n",
        "* https://pytorch.org/docs/stable/notes/randomness.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nSqNzof1lTJ"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, LabelField, BucketIterator\n",
        "\n",
        "SEED = 123\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.set_deterministic(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y1_GyXg22f6"
      },
      "source": [
        "### torchtextのフィールド\n",
        "* TEXTフィールドと、LABELフィールドという２種類のFieldオブジェクトのインスタンスを作る。\n",
        " * Fieldクラスの詳細については[ここ](https://github.com/pytorch/text/blob/master/torchtext/data/field.py)を参照。\n",
        "* TEXTフィールドは、テキストの前処理の仕方を決めておくのに使う。\n",
        " * tokenizerは、デフォルトでは単にstring型のsplitメソッドを適用するだけになる。これは高速だが、tokenizationとしては雑。\n",
        "* LABELフィールドは、ラベルの前処理に使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjq8oooE2uQY"
      },
      "source": [
        "TEXT = Field(tokenize=\"spacy\")\n",
        "LABEL = LabelField()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEq23GS3Vxl"
      },
      "source": [
        "### IMDbデータセットをダウンロードした後、前処理しつつ読み込む\n",
        "* ダウンロードはすぐ終わるが、解凍に少し時間がかかる。\n",
        "* また、TEXTフィールドでspaCyのtokenizationを使うように設定したので、少し時間がかかる。\n",
        " * string型のsplitメソッドでtokenizeすると、時間はあまりかからない。（そのかわり、やや雑なtokenizationになる。）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzgVXf3G3YPI"
      },
      "source": [
        "train_valid_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0sltPjT3j36"
      },
      "source": [
        "### 最初の文書を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrXwYMVH3orf"
      },
      "source": [
        "print(train_valid_data[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRMuAOum3rB5"
      },
      "source": [
        "print(train_valid_data[0].label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgZgQbyD3u9D"
      },
      "source": [
        "### テストセット以外の部分を訓練データと検証データに分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2FtnEKZ32hM"
      },
      "source": [
        "train_data, valid_data = train_valid_data.split(split_ratio=0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fzsi9ZC36eR"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsLNP7pGaNtp"
      },
      "source": [
        "print(train_data[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oXz2lvB37Vm"
      },
      "source": [
        "### データセットの語彙とラベルを作る\n",
        "* TEXTラベルのほうでは、最大語彙サイズを指定する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBQeD7yC37x4"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000 # この値は適当。\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iv6RSh3HmLf"
      },
      "source": [
        "なぜ語彙サイズが25,000ではなく25,002なのかについては、少し下の説明を参照。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWuYQthC4Ml8"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW4eR-K44Rba"
      },
      "source": [
        "### 出現頻度順で上位２０単語を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jan98ffr4PXP"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKQojOuv4Z38"
      },
      "source": [
        "### 単語ID順に最初の１０単語を見てみる\n",
        "* IDのうち、0と1は、未知語とパディング用の単語という特殊な単語に割り振られている。\n",
        " * 未知語は`<unk>`という特殊な単語に置き換えられる。これのIDが0。\n",
        " * パディングとは、長さが不揃いの複数の文書を同じミニバッチにまとめるとき、すべての文書の長さを無理やりそろえるため、文書末尾に特殊な単語（元々の語彙にない、人工的に用意した単語）を追加すること。\n",
        " * パディング用の単語が`<pad>`になっているのは、上のほうで使ったFieldクラスのインスタンスを作るときのデフォルトの値がこの`<pad>`になっているため。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXRT3g4Xad"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJfHTdR4qd4"
      },
      "source": [
        "### ラベルのほうのIDを確認する\n",
        "* こちらはnegとposに対応する２つのIDしかない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI7Pz_6R4bYM"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14_znTjp4w5s"
      },
      "source": [
        "### ミニバッチを取り出すためのiteratorを作る\n",
        "* ミニバッチのサイズを指定する。\n",
        " * ミニバッチのサイズは、性能を出すためにチューニングすべきハイパーパラメータのひとつ。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUED86Jb4tUy"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator = data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device,\n",
        "                                     sort_within_batch=True, shuffle=True, sort_key=lambda x: len(x.text))\n",
        "valid_iterator = data.BucketIterator(valid_data, batch_size=BATCH_SIZE, device=device)\n",
        "test_iterator = data.BucketIterator(test_data, batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45QA7ncg_Qv"
      },
      "source": [
        "### ミニバッチの中身を確認する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAW9Ec5q6BQO"
      },
      "source": [
        "* 訓練データのiteratorを回してミニバッチをすべて取得してみる\n",
        " * ミニバッチのshapeを表示してみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpn4tfWl42kY"
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  print(i, batch.text.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWW1np1P6OQg"
      },
      "source": [
        "* ミニバッチの形は、[ミニバッチ内での最大文書長, ミニバッチのサイズ]になっていることに注意！\n",
        " * ミニバッチのサイズが最初に来ているのではない！\n",
        " * [ミニバッチのサイズ, ミニバッチ内での最大文書長]という形にしたいなら、テキストのfieldを作るとき以下のようにする。\n",
        "\n",
        "__`TEXT = data.Field(tokenize=\"spacy\", batch_first=True)`__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHytOsiSUdeS"
      },
      "source": [
        "* 上記のループを抜けたあとには、変数batchには最後のミニバッチが代入されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d78vJW616H7m"
      },
      "source": [
        "batch.text.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHMHkR73VuCD"
      },
      "source": [
        "* このミニバッチに含まれる文書のうち、最初の文書の単語ID列を表示させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tZLm0hQVjZE"
      },
      "source": [
        "print(batch.text[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcXV1WLCdax1"
      },
      "source": [
        "* このミニバッチに含まれる文書のうち、最初の文書の単語ID列を単語列に戻したものを表示させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjaS-Mjadf63"
      },
      "source": [
        "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:, 0]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uoslpyTgz8w"
      },
      "source": [
        "* このミニバッチに含まれる文書のうち、最後の文書の単語ID列を表示させてみる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcdyIhK0TUac"
      },
      "source": [
        "print(batch.text[:, BATCH_SIZE-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtDXRKPMT9KW"
      },
      "source": [
        "最後の文書の末尾は「1」で埋められていることが分かる。\n",
        "\n",
        "この1は、パディング用単語のIDだったことを想起されたい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDzk2ghCUD8N"
      },
      "source": [
        "ミニバッチに含まれる文書の長さを調べると、文書が文書長の降順に並べられていることが分かる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PutP_EU4Tca-"
      },
      "source": [
        "(batch.text != 1).sum(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PDZlF0O6doP"
      },
      "source": [
        "## 07-02 MLPによる文書分類の準備\n",
        "* 今回は、ごく簡単なMLPで文書分類をする。\n",
        "* 文書中の全単語トークンの埋め込みベクトルの平均を、MLPの入力とする。\n",
        " * 当然、語順の情報は使われない。\n",
        " * つまり、bag-of-wordsモデルになっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpel2i46gbD"
      },
      "source": [
        "### 定数の設定\n",
        "* 単語埋め込みベクトルの次元数は128にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQPXVLC66NUM"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "NUM_CLASS = len(LABEL.vocab)\n",
        "EMBED_DIM = 128\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "print(f'語彙サイズ {INPUT_DIM}, クラス数 {NUM_CLASS}, 単語埋め込み次元 {EMBED_DIM}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsuHjuNp6tvt"
      },
      "source": [
        "### モデルを定義する前にPyTorchの単語埋め込みがどんなものかを見てみる\n",
        "* 埋め込みとは、単語IDから単語ベクトルへのマッピング。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3J7TzxFVMsR"
      },
      "source": [
        "* 以下のように、語彙サイズと埋め込みの次元数を指定しつつ、torch.nn.Embeddingのインスタンスを作ればよい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP7jJVYT6tBg"
      },
      "source": [
        "embed = nn.Embedding(INPUT_DIM, EMBED_DIM, padding_idx=PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUl6lR8JVWTu"
      },
      "source": [
        "* パディング用の単語の埋め込みはゼロベクトルになる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZCr9Ll61m8"
      },
      "source": [
        "print(embed(torch.tensor([21,1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN0kM3urr-Il"
      },
      "source": [
        "* 埋め込みの効果を見てみる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQ0FqJRcr6Vo"
      },
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "  print(i, embed(batch.text).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyngitc78hv"
      },
      "source": [
        "### モデルの定義\n",
        "* MLP（多層パーセプトロン）だが、入り口に単語埋め込み層が挿入されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asdLYng7DOu"
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.embed(text)\n",
        "    x = x.mean(0) # 文書に含まれる全単語トークンの単語ベクトルの平均\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foU72cB48IO9"
      },
      "source": [
        "### モデルを作る\n",
        "* モデル（のインスタンス）をGPUに移動させている点に注意。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0BHCGAZ8F18"
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wylQOq8N8cqI"
      },
      "source": [
        "### 損失関数とoptimizerとschedulerを作る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw34INS78cIW"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilWLfu8Z8MzW"
      },
      "source": [
        "### 訓練用の関数\n",
        "* 最初の`model.train()`に注意。こうやって、モデルを訓練モードに設定する。\n",
        " * 例えば、dropoutを含むモデルなど、訓練時と評価時で、ふるまい方を変える必要があるときがあるため、こういうことをする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR2R4Lqh8J7n"
      },
      "source": [
        "def train(data_iterator, model, optimizer, scheduler, criterion):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  for batch in data_iterator:\n",
        "    optimizer.zero_grad()\n",
        "    text, cls = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, cls)\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc += (output.argmax(1) == cls).float().mean().item()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "  num_batch = len(data_iterator)\n",
        "  return train_loss / num_batch, train_acc / num_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftuX8e1W8iRh"
      },
      "source": [
        "### 評価用の関数\n",
        "* 最初の`model.eval()`に注意。こうやって、モデルを評価モードに設定する。\n",
        " * 例えば、dropoutを含むモデルなど、訓練時と評価時で、ふるまい方を変える必要があるときがあるため、こういうことをする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGUnsJlq8Ue3"
      },
      "source": [
        "def test(data_iterator, model, criterion):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in data_iterator:\n",
        "    text, cls = batch.text, batch.label\n",
        "    with torch.no_grad():\n",
        "      output = model(text)\n",
        "      loss = criterion(output, cls)\n",
        "      loss += loss.item()\n",
        "      acc += (output.argmax(1) == cls).float().mean().item()\n",
        "\n",
        "  num_batch = len(data_iterator)\n",
        "  return loss / num_batch, acc / num_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8o_jDAg8osP"
      },
      "source": [
        "## 07-03 分類器の訓練と検証セットでの評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJJFv4k-8mH1"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs // 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
        "    break\n",
        "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
        "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPux8PReWTXG"
      },
      "source": [
        "## 07-04 再検討\n",
        "* 訓練データ上での分類精度がほぼ100%になってしまっている。\n",
        "* 検証データでの分類精度と大きな差があり、明らかにオーバーフィッティング。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jMgtmoWkty"
      },
      "source": [
        "### ドロップアウトを使う\n",
        "* モデルのインスタンスを作るときにdropoutの確率を引数pで指定できるようにする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khps3ZuBWntq"
      },
      "source": [
        "class EmbedTextSentiment(nn.Module):\n",
        "  def __init__(self, embed_dim, num_class, vocab_size, padding_idx, p=0.0):\n",
        "    super(EmbedTextSentiment, self).__init__()\n",
        "    self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "    self.dropout = nn.Dropout(p=p)\n",
        "    self.fc1 = nn.Linear(embed_dim, 500)\n",
        "    self.fc2 = nn.Linear(500, 100)\n",
        "    self.fc3 = nn.Linear(100, num_class)\n",
        "\n",
        "  def forward(self, text):\n",
        "    x = self.dropout(self.embed(text)) #埋め込み層の直後にdropout\n",
        "    x = x.mean(0)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVXbkt6qXxNt"
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXkBDXc6X1mp"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs // 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
        "    break\n",
        "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
        "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu3Y-wjwb0po"
      },
      "source": [
        "### L２正則化を使う\n",
        "* optimizerのweight_decayパラメータを0より大きな値にする。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmxEuSFJazCJ"
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Zr2S7ga3J4"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs // 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
        "    break\n",
        "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
        "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHA64UTdmBj"
      },
      "source": [
        "### early stopping\n",
        "* dev setでのaccuracyが4回連続で最高値を下回ったら訓練を終えることにする。\n",
        "* early stoppingの実現については、PyTorch Lightningを使う手もある。\n",
        " * https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zclQnVdlVZ"
      },
      "source": [
        "model = EmbedTextSentiment(EMBED_DIM, NUM_CLASS, INPUT_DIM, padding_idx=PAD_IDX, p=0.5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3E_I5sRc3FF"
      },
      "source": [
        "patience = 4\n",
        "early_stop_count = 0\n",
        "best_valid_acc = 0.0\n",
        "\n",
        "MIN_N_EPOCHS = 10 # 最低このエポック数は実行する\n",
        "N_EPOCHS = 50 # エポック数を増やしておく\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(train_iterator, model, optimizer, scheduler, criterion)\n",
        "  valid_loss, valid_acc = test(valid_iterator, model, criterion)\n",
        "\n",
        "  secs = int(time.time() - start_time)\n",
        "  mins = secs // 60\n",
        "  secs = secs % 60\n",
        "\n",
        "  print(f'Epoch {epoch + 1} | time in {mins:d} minutes, {secs:d} seconds | ', end='')\n",
        "  for param_group in optimizer.param_groups:\n",
        "    print(f'lr={param_group[\"lr\"]:.6f}')\n",
        "    break\n",
        "  print(f'\\tLoss: {train_loss:.5f}(train)\\t|\\tAcc: {train_acc * 100:.2f}%(train)')\n",
        "  print(f'\\tLoss: {valid_loss:.5f}(valid)\\t|\\tAcc: {valid_acc * 100:.2f}%(valid)')\n",
        "\n",
        "  # early stopping\n",
        "  if epoch + 1 > MIN_N_EPOCHS:\n",
        "    if best_valid_acc <= valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      early_stop_count = 0\n",
        "    else:\n",
        "      early_stop_count += 1\n",
        "      if early_stop_count == patience:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRvkncN09MKk"
      },
      "source": [
        "## 07-05 テストセット上で評価\n",
        "* 見つけ出したベストな設定を使って、テストセット上での最終的な評価をおこなう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gHj4x38y8h"
      },
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_iterator, model, criterion)\n",
        "print(f'\\tLoss: {test_loss:.5f}(test)\\t|\\tAcc: {test_acc * 100:.2f}%(test)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M_VQ1xhcWq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}